# Pre-trained Language Models

The intuition behind pre-trained language models is to create a black box which understands the language and can then be asked to do any specific task in that language.

| Title | Description |
| ----- | ----------- |
| ParsBERT<br>[[website]](https://github.com/hooshvare/parsbert) [[paper]](https://arxiv.org/abs/2005.12515) | ParsBERT is a monolingual language model based on Googleâ€™s BERT architecture. This model is pre-trained on large Persian corpora with various writing styles from numerous subjects (e.g., scientific, novels, news) with more than 3.9M documents, 73M sentences, and 1.3B words. |
| ParsPer<br>[[website]](https://sites.google.com/site/mojganserajicom/home/parsper) [[download]](https://sites.google.com/site/mojganserajicom/home/parsper/parsper-1/model_ParsPer.tar?attredirects=0&d=1) | ParsPer is created by training the graph-based MateParser on the entire Uppsala Persian Dependency Treebank (UPDT) with a selected configuration. |
